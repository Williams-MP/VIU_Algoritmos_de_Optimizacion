{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9177a8-9440-48b3-b20c-dfffeb9bc40b",
   "metadata": {},
   "source": [
    "# 1. Comprensión del Problema\n",
    "\n",
    "En esta fase inicial, el objetivo es entender la naturaleza de los datos y verificar que la carga de imágenes y etiquetas es correcta.\n",
    "\n",
    "* **Contexto:** El glaucoma es una enfermedad neurodegenerativa que afecta al nervio óptico. ***Es una de las principales causas de ceguera en el mundo y su diagnóstico es complejo, ya que requiere de múltiples pruebas debido a que no existe un único estándar de oro***.\n",
    "* **Preparación previa:** ***En un flujo de trabajo inicial se deberían obtener las máscaras mediante técnicas de segmentación, pero en este ejercicio ya disponemos de ellas (ground truth) para facilitar el análisis***.\n",
    "* **Lectura de metadatos:** Cargamos el archivo Excel que contiene las etiquetas y las rutas de las imágenes. ***Contamos con un dataset balanceado de 189 muestras (93 con glaucoma y 96 sanas), lo cual es ideal para evitar sesgos hacia una clase específica durante el entrenamiento***.\n",
    "* **Inspección visual:** Seleccionamos muestras aleatorias para validar que las máscaras de la retina y de la capa de fibras nerviosas (RNFL) coinciden con la imagen original. ***El grosor de la capa RNFL es el biomarcador más crítico para el diagnóstico clínico de esta enfermedad***.\n",
    "\n",
    "### Implementación en Python\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Extraer información de interés\n",
    "# Leemos la información del documento Excel (ajustar separador si es necesario)\n",
    "dataFrame = pd.read_csv('glaucoma.csv', sep=';') \n",
    "\n",
    "# Extraer las clases y su frecuencia para verificar el balanceo\n",
    "clases, frc = np.unique(dataFrame.Class, return_counts=True)\n",
    "print(f\"Clases: {clases}\")\n",
    "print(f\"Frecuencias: {frc}\")\n",
    "\n",
    "# 2. Selección de muestra aleatoria de nuestra base de datos\n",
    "idx = random.randint(0, len(dataFrame) - 1)\n",
    "path_img = dataFrame.iloc[idx]['Path']\n",
    "path_mask = dataFrame.iloc[idx]['MaskPath']\n",
    "\n",
    "# Usamos cv2 para leer las imágenes y verlas en python (escala de grises)\n",
    "img = cv2.imread(path_img, 0)\n",
    "rnfl_mask = cv2.imread(path_mask, 0)\n",
    "\n",
    "# 3. Visualización: Leemos las máscaras y ploteamos los contornos sobre la imagen\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(img, cmap='gray')\n",
    "# Dibujamos el contorno de la máscara sobre la imagen original\n",
    "plt.contour(rnfl_mask, colors='red', linewidths=0.5) \n",
    "plt.title(f\"Muestra: {idx} - Diagnóstico: {dataFrame.iloc[idx]['Class']}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75968ecc-bafb-42fe-96c9-103108bc4288",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e30b4077-fb49-4da0-9a78-076c5a34c31d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bff892b5-6e8c-4ca8-948f-b24043e5d057",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc772cab-8441-4a1c-8a0c-10ebd325ee8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0784522-a8a8-47c8-bef8-cf3a81bff485",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d06552f-2f87-448a-b68f-7298bfdf14e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd63599f-d59c-4846-bdc3-0d18014a2b38",
   "metadata": {},
   "source": [
    "# Compresión del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67b40f-2caf-4f78-86ca-509558d1fdc8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06630f43-8ef3-4a0c-9a42-8dfe30a5f38e",
   "metadata": {},
   "source": [
    "\n",
    "- Objetivo\n",
    "- Contexto\n",
    "    - Glaucoma\n",
    "- Procedimiento diagnóstico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a4892-817a-4658-a7ff-7383d986e439",
   "metadata": {},
   "source": [
    "1. Comprensión del Problema\n",
    "En esta fase inicial, el objetivo es entender la naturaleza de los datos y verificar que la carga de imágenes y etiquetas es correcta.\n",
    "\n",
    "Contexto: El glaucoma es una enfermedad neurodegenerativa que afecta al nervio óptico. Es la principal causa de ceguera irreversible en el mundo y su diagnóstico es complejo porque no depende de una sola prueba.\n",
    "\n",
    "Preparación previa: En un flujo de trabajo inicial se deberían obtener las máscaras mediante segmentación, pero en este ejercicio ya disponemos de ellas (ground truth).\n",
    "\n",
    "Lectura de metadatos: Cargamos el archivo Excel que contiene las etiquetas y rutas. El dataset está balanceado (93 glaucoma / 96 sanos), por lo que no sufriremos sesgos de clase durante el entrenamiento.\n",
    "\n",
    "Inspección visual: Seleccionamos muestras aleatorias para validar que las máscaras de la retina y de la capa de fibras nerviosas (RNFL) coinciden con la imagen original. Recordemos que el grosor de la RNFL es el biomarcador clave para detectar el daño.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd72dc2-aeee-430a-8023-6347b77ce2be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65342b27-7bb3-408d-be87-799499d8ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer información de interés\n",
    "import numpy as no\n",
    "clases, frc = np.uniques(dataFrame.Class, return_Counts=True)\n",
    "Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab4b41-296e-42b1-bfd7-56d4230d5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccion una muestra aleatoria de nuestra base de datos\n",
    "import random\n",
    "import cv2\n",
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebcd3ed-30e9-4f0c-a366-719c0f776c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leesmos las máscaras y ploteamos los contonos sobre la imagen original\n",
    "rnfl_mask = cv2.impread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e2c1a-b2b3-4af7-8615-abd7641641ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c10a45b-4d4a-4a27-8746-b359ead2337c",
   "metadata": {},
   "source": [
    "# Partición externa de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88db0e-8dd3-4e72-85f8-abdb76502d98",
   "metadata": {},
   "source": [
    "- Separar conjutno de test z conjunto de entrenamiento\n",
    "- leer la información del documento Excel original\n",
    "- Hacemos la particion externa de los datos ()\n",
    "    -alternativa-->  train, test = train_test_split(dataFrame, test_size=0.2, shuffle=True, random_state=42) # hold-out\n",
    "Se crean los subconjuntos\n",
    "- Miramos las muestras que se han subdividido\n",
    "- Pasamos a aleatorizar las muestras\n",
    "- Por ultimo guardmos los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9af90-da6b-42d4-b74d-e8547a0e013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partición externa de los datos\n",
    "from sklearn.model_Selection import train_test_split, KFold\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43633601-c133-438b-8c15-679b7bdb2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aleatorizar los dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddfbd8f-1555-4bf9-93ed-a2066083ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gurdado de los modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26519b6b-48cf-4e75-89fb-8878fb09a7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19476fd0-9cbb-4db7-9fc9-915a9010930a",
   "metadata": {},
   "source": [
    "# Extracción de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95aac70-f490-40d9-be6e-2d9692788fda",
   "metadata": {},
   "source": [
    "- Cogemos los archivos csv extraidos antes, de entrenamiento y de test y los cargamos.\n",
    "- Ponemos las muestras en un bucle for para extraer las caracteristicas que queremos. COmenzamos con las caracteristicas de una muestra, y una vez las tengamos, ampliamos el for a todas las muestras.\n",
    "- Importamos las libreria cv2 para leer las imagen y le indicamos donde estan. (ha usado0 para usar escala de grises)\n",
    "- Importmaos mathplotlib.pyplot para visualizar\n",
    "- Vislualizamos los datos que nos interesan y buscamos la información de interes.\n",
    "- Aplicamos estadisticos unidimensionsanes\n",
    "        - bucle for para detectar las posicions donde la mascra esta en balnco (=255). Se extraen los indices\n",
    "        - se pasan a un numpy array\n",
    "    - Caracteristicas basadas en medidads de tendencia central (media, mediana)\n",
    "    -  Caracteristicas basadas en medidads de dispersion.(desviacion estandar)\n",
    "    -  Caracteristicas de distribucion (importar scipy stats). Curtosis y asimetria\n",
    "    -  Otras caracteristicas (minimo, y maximo)\n",
    "    -  Agrupamos las caracteristicas de unidimensionales\n",
    "- Caracteristicas bidimensionales\n",
    "    - Extraer el bounding box (skiimage.mneasure , regioprops)\n",
    "    - Y hacemos un crop para mantener la parte que nos interesa\n",
    "    - Extrar caracteristicas de textura (miden cambio intensidad)\n",
    "        - matriy concurencia lray level (GLCM) grecomtriy, greycrops\n",
    "            - extraemos en contraste de la matriz\n",
    "            - disimulitud\n",
    "            - homogeneidad\n",
    "            - ASM\n",
    "            - energia\n",
    "            - correlacion\n",
    "            -  \n",
    "        - Local binary patterns (LBP) \n",
    "            -  Definir radio r y numero de vecinos, y el método\n",
    "            -  pasar a uint8\n",
    "            -  sacar el histograma\n",
    "    -  Agruapamsos las caratreisticas t te etxtura\n",
    "- Matriz de datos construidas, pero ahora hay que poner las etiquestas..\n",
    "- Finalmente concatenamos todas la caacteristicas mas la variable de la etiqueta.\n",
    "- Despues de haber extraido las caratceristica con una muestra, pues enxtendemos a todas las muetras\n",
    "- creamos la funcion de extraccion para poder usar con otro dataframe, y que devuelva la matriz de datos. Entonces se hara para el test y el train.\n",
    "- Creamos la carpeta \"features\" por ejmplo, donde se almaceranaran las estdisticas en un archivos npy. Creamos el archivo con las caracteristicas para train y test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d2467-5353-41a3-8225-11b52d7aabb0",
   "metadata": {},
   "source": [
    "# Selección de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba2264-f64e-4909-896a-744d60168bab",
   "metadata": {},
   "source": [
    "- Cargar los datos de entrenamiento en una variable ej. train_matrix\n",
    "- Seleccionas las features y target  (última comlumna deberia ser la clase)\n",
    "- Procedemos a la estandarizacion de los datos de entrenamiento\n",
    "    - sklearn.preprocessing import StandardScaler\n",
    "    - extraer la mu y la sigma\n",
    "- Selecion de los atributos (caracteristicas)\n",
    "- Estudiar si las varibale ssiguen una distribucion normal de meido 0 disviacion tipica 1 --> n(0,1)\n",
    "- scipy.stats import kstets # PRueba de Komorov-Smirnov\n",
    "- defninir Nivel de confinaza para elaborar un contraste de hipotesis\n",
    "- extramos un p-valor y los comparamos\n",
    "- En un bucle for analizamos las caracteristicas\n",
    "- Si pvalor menor o igual que alpfa rechazamos la hipoesis nula\n",
    "- Identificar las variables que siguen o no esa normalidad\n",
    "- Estudiad la capacidad discriminativa de los atributis en funcion de su distribucion\n",
    "    - t-stundet (compracion de medias) para los que siguen distribucion normal\n",
    "    - mannhwhitneyu (comparacion de medianas). Interesa la qe las medianas sean diferentes, para poder discriminar. Es decir, podemos ver que cacrateristicas toman valores difertnes dependiendo de la clase. Entonces, si no discriminatoria, entonces no nos ayuda.\n",
    "    - Es importante miran que caracetristicas son las discriminatorias\n",
    "    - Contraste de hipotesis, estudiar el poder discriminatoria de las caracteristicas\n",
    "    - H0: independencia entre la caracterirca y la clase.\n",
    "    - if pvalue<=alpha, se rechaza la HOy , por tanto, asumimos la depencidia entre la caracteristica y la calse\n",
    "    - Else , no hay evidendcia para rechazar la Ho y, por tanto, asumimos que la carcteristica y la calse son independientes.\n",
    "- De aqqui elemininamos las variables que no son discrimnatorias.\n",
    "-  borrar las mu y la sigma donde tengo variables que no son discriminatorias\n",
    "-  VIsualizacion de ticks (diagrama caja bigotes)\n",
    "-  HAcemos la funcion draw_boxplot para hacer el diagrama caja bigotes.\n",
    "-  Cuando mas separadas las cajas menjor, porque la caracteristica es un buen discriminador.\n",
    "-  Ahora buscaremos una correlacion entre las carataeristicas, para evitar que den la misma informacion y asi reducir el coste computacional.\n",
    "-  Analisis de correlacion para ver la dependnecia entre pares de variables\n",
    "-  Creacion de la matriz de correlacion\n",
    "-  Fiajmos un umbral de correlacion para filtrar for ejemplo 0,9 (90%)\n",
    "-  definimos un indice que nos dire donde se pasa eese eumbral en la matriz.\n",
    "-  Cremoa matriz triangular superior para saber donde estan las variables correladas. Lo pasamos a  uint8.\n",
    "-  buscamos cdonde las columnes valen 1, para encontrar las varibaes\n",
    "-  Eliminamos las varibales correlacionadas\n",
    "-  Guardamos la matrz de características final para el train y el test\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b6f92-7e22-454e-95c5-7042ccfe686c",
   "metadata": {},
   "source": [
    "# Modelado\n",
    "- Cargar los datos de entrenamiento\n",
    "- Definir los modelos de clasficacion\n",
    "    - Utilizar el perceptron multicapa, que permite clasificar datos de manera no lineal durante el forward y backpropagation\n",
    "    -  Los algortimos multipcapa del perceptron se puede usar para clasificacion y regresion\n",
    "- Importan el modelo linea para usar el logisticregession y el multicapa MLP\n",
    "-  DefinirLOGR penalyt=12, solver= lbfgs, max iter =100, random state:42\n",
    "-  Definir MLP Hidden_layer_sizesm funciones activation (relu, solver adam, batch suze= auto,m laerning rate=constant, learning rate_init=0.001, max_iter=200 (a veces se neceista mas), rando_state=42)\n",
    "- Cross-validaten interno en k=5 bolsas (cross_val_score, Kfold)\n",
    "    - gurdamos los resultados en un diccionaio\n",
    "    - A partir de aqui vamos ajustamo parametros para obtener la mejor accuracy\n",
    "- Construimos el modelos definitio\n",
    "    - Atributos se obtienen durante el entrnamiento\n",
    " - Guardar modelos\n",
    "     - Se hace en pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4fef89-d447-436c-9444-bf2ef487d91a",
   "metadata": {},
   "source": [
    "# Evaluación de resultados.\n",
    "- Cargar los datos del test\n",
    "- Se cargan los modelos entrenados (Pickel)\n",
    "- Extraer las predicciones\n",
    "- Evaluar diversas metricas de clasificacion\n",
    "    - Precision\n",
    "    - Sensibilidad\n",
    "    - F1-score\n",
    "    - accuracy\n",
    "    - AUC\n",
    "- Se hace un for in range (0,2) para LOGR\n",
    "- se ponen todos los datos en tuplas para ver los resultados\n",
    "- Se hace la matriz de confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322365fb-5577-429c-a1af-edcf9e56e4f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5b9a9-81f7-45a3-bbc8-1538a0cdc0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
